{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EGIUO8BGRyD3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","def split_data(source_dir, train_dir, val_dir, val_ratio=0.15):\n","    # Create directories if they don't exist\n","    for directory in [train_dir, val_dir]:\n","        for subdir in ['NG', 'OK']:\n","            os.makedirs(os.path.join(directory, subdir), exist_ok=True)\n","\n","    # Split the data\n","    for subdir in ['NG', 'OK']:\n","        files = os.listdir(os.path.join(source_dir, subdir))\n","        random.shuffle(files)\n","\n","        n_total = len(files)\n","        n_val = int(n_total * val_ratio)\n","\n","        val_files = files[:n_val]\n","        train_files = files[n_val:]\n","\n","        # Copy files to validation set\n","        for file in val_files:\n","            shutil.copy(os.path.join(source_dir, subdir, file), os.path.join(val_dir, subdir, file))\n","\n","        # Copy files to train set\n","        for file in train_files:\n","            shutil.copy(os.path.join(source_dir, subdir, file), os.path.join(train_dir, subdir, file))\n","\n","# Define directories\n","source_dir = '/content/drive/MyDrive/Train'\n","train_dir = '/content/drive/MyDrive/Train/train'\n","val_dir = '/content/drive/MyDrive/Train/val'\n","\n","# Split the dataset\n","split_data(source_dir, train_dir, val_dir)"],"metadata":{"id":"kIRKAEmn_OzR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oypilQy_b-e4"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","source_dir = '/content/drive/MyDrive/Train'\n","train_dir = '/content/drive/MyDrive/Train/train'\n","val_dir = '/content/drive/MyDrive/Train/val'\n","test_dir = '/content/drive/MyDrive/Test'\n","\n","# Define the autoencoder model\n","class Autoencoder(nn.Module):\n","    def __init__(self):\n","        super(Autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # b, 64, 16, 16\n","            nn.ReLU(True),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # b, 128, 8, 8\n","            nn.ReLU(True),\n","            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # b, 256, 4, 4\n","            nn.ReLU(True),\n","            nn.Flatten(),  # b, 256*4*4\n","            nn.Linear(256*4*4, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 256),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Linear(256, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 256*4*4),\n","            nn.ReLU(True),\n","            nn.Unflatten(1, (256, 4, 4)),  # b, 256, 4, 4\n","            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # b, 128, 8, 8\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # b, 64, 16, 16\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=2, padding=1, output_padding=1),  # b, 3, 32, 32\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# Prepare data\n","transform = transforms.Compose([\n","    transforms.Resize((32, 32)),\n","    transforms.ToTensor(),\n","])\n","\n","# Load datasets\n","train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n","val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n","test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Initialize the model, loss function, and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = Autoencoder().to(device)  # Move model to GPU if available\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop with validation\n","\n","# Initialize lists to store loss values\n","train_losses = []\n","val_losses = []\n","\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0\n","    for images, _ in train_loader:\n","        images = images.to(device)\n","\n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, images)\n","\n","        # Backward pass and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    # Store the average training loss for this epoch\n","    train_losses.append(train_loss / len(train_loader))\n","\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for images, _ in val_loader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, images)\n","            val_loss += loss.item()\n","\n","    # Store the average validation loss for this epoch\n","    val_losses.append(val_loss / len(val_loader))\n","\n","    # Print epoch results\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}')\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'autoencoder.pth')\n","\n","# Plot the loss curves\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n","plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss Curves')\n","plt.legend()\n","plt.show()\n","\n","\n","# Load the trained model\n","model.load_state_dict(torch.load('autoencoder.pth'))\n","model.eval()\n","\n","# Function to compute reconstruction error\n","def reconstruction_error(x, x_hat):\n","    return torch.mean((x - x_hat)**2, dim=(1, 2, 3))\n","\n","# Compute reconstruction errors for test set\n","errors = []\n","labels = []\n","with torch.no_grad():\n","    for images, cls in test_loader:\n","        images = images.to(device)\n","        outputs = model(images)\n","        error = reconstruction_error(images, outputs)\n","        errors.extend(error.cpu().numpy())\n","        labels.extend(cls.cpu().numpy())\n","\n","# Convert to numpy arrays\n","errors = np.array(errors)\n","labels = np.array(labels)\n","\n","# Determine threshold (e.g., using the 95th percentile of 'OK' reconstruction errors)\n","ok_errors = errors[labels == 0]  # 0 is the label for 'OK' images\n","threshold = np.percentile(ok_errors, 95)\n","\n","# Classify images as normal or anomalous\n","predictions = (errors > threshold).astype(int)\n","\n","# Compute metrics\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n","\n","accuracy = accuracy_score(labels, predictions)\n","precision = precision_score(labels, predictions)\n","recall = recall_score(labels, predictions)\n","f1 = f1_score(labels, predictions)\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1-score: {f1:.4f}\")\n","\n","\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(labels, predictions)\n","\n","# Display the confusion matrix\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['OK', 'Defective'])\n","disp.plot(cmap=plt.cm.Blues)\n","disp.plot()\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","\n","\n","# Calculate precision and recall for different thresholds\n","precision, recall, thresholds = precision_recall_curve(labels, errors)\n","\n","# Plot the precision-recall curve\n","plt.figure(figsize=(10, 5))\n","plt.plot(recall, precision, marker='.')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.grid(True)\n","plt.show()\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"wUMt19hrCAeM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVTrh1Kfb-VQ"},"outputs":[],"source":["import numpy as np\n","from sklearn.ensemble import IsolationForest\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, PrecisionRecallDisplay, roc_curve\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import torch\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),\n","    transforms.ToTensor()\n","])\n","\n","# Load datasets\n","train_dir = '/content/drive/MyDrive/Train'\n","test_dir = '/content/drive/MyDrive/Test'\n","\n","train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n","test_dataset = datasets.ImageFolder(test_dir, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Extract features from images\n","def extract_features(dataloader):\n","    features = []\n","    labels = []\n","    for images, batch_labels in dataloader:\n","        # Flatten the images\n","        batch_features = images.view(images.size(0), -1).numpy()\n","        features.append(batch_features)\n","        labels.extend(batch_labels.numpy())\n","    return np.vstack(features), np.array(labels)\n","\n","# Extract features\n","X_train, y_train = extract_features(train_loader)\n","X_test, y_test = extract_features(test_loader)\n","\n","# Train Isolation Forest\n","contamination = 0.1  #  expectation of anomaly ratio\n","clf = IsolationForest(contamination=contamination, random_state=42)\n","clf.fit(X_train)\n","\n","# Predict anomalies\n","# Isolation Forest returns 1 for inliers and -1 for outliers, so we need to flip these\n","y_pred_train = clf.predict(X_train)\n","y_pred_test = clf.predict(X_test)\n","\n","# Convert predictions to binary (0: normal, 1: anomaly)\n","y_pred_train = (y_pred_train == -1).astype(int)\n","y_pred_test = (y_pred_test == -1).astype(int)\n","\n","# Assuming 'NG' (anomaly) is labeled as 1 and 'OK' (normal) as 0\n","y_true_train = (y_train == train_dataset.class_to_idx['NG']).astype(int)\n","y_true_test = (y_test == test_dataset.class_to_idx['NG']).astype(int)\n","\n","# Compute metrics\n","accuracy = accuracy_score(y_true_test, y_pred_test)\n","precision = precision_score(y_true_test, y_pred_test)\n","recall = recall_score(y_true_test, y_pred_test)\n","f1 = f1_score(y_true_test, y_pred_test)\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1-score: {f1:.4f}\")\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_true_test, y_pred_test)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['OK', 'NG'])\n","disp.plot(cmap='Blues')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","# Precision-Recall Curve\n","precision_vals, recall_vals, thresholds = precision_recall_curve(y_true_test, clf.decision_function(X_test))\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall_vals, precision_vals, marker='.')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.show()\n","\n","# ROC Curve\n","fpr, tpr, _ = roc_curve(y_true_test, clf.decision_function(X_test))\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, marker='.')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyNiCN2rjCnJYtIIx7INVCVU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}